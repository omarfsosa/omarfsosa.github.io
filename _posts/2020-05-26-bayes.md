---
layout: single
title:  "Bayesian statistics for the gradient-based data scientist"
description: "Intro to Bayesian statistics for Machine Learning"
date:   2020-05-26
mathjax: true
plotly: true
tags: [bayesian statistics, python, pystan]
---
<style>
.orange_dot {
  height: .7em;
  width: .7em;
  background-color: #F08636;
  border-radius: 50%;
  display: inline-block;
}

.blue_dot {
  height: .7em;
  width: .7em;
  background-color: #3876AF;
  border-radius: 50%;
  display: inline-block;
}

.red_text {
  color: red;
}
</style>
Depending on the stage of your career as a data scientist, Bayesian statistics might have made its appearance in a variety of contexts. Today, being a data scientist has such a wide meaning that, most likely, you've found that not knowing Bayesian statistics has not prevented you from doing your job. This might have you wonder why people make such big fuzz about it.

Back in my physicist days, my formation involved a lot of statistics, but there was never any mention of _Bayesian_ statistics. When I first heard the term my thought was "You mean like Bayes rule? ... Sure I know Bayesian statistics.".  When I finally was properly introduced to the topic, I learned that there are two schools of statistics: Bayesian and frequentist. As it was explained to me, the philosophical differences between these to approaches mean that you can ask a Bayesian stuff such as "Hey, what is the probability that I have a missing twin?" and have a peaceful discussion about it. A frequentist, on the other hand, will presumably scold you for your nonsense question. Understanding the philosophical differences between the two schools of thought was ok, but it didn't quite take me all the way. It gave me the impression that, if I wished to talk about probability with anyone, I should first ask if they are frequentist or Bayesian, so that I might not say something offensive. It took me a while to understand what the practical implications were. For example, when I'm doing my usual stuff (i.e. using xgboost for absolutely everything), am I taking a frequentist or a Bayesian approach? Does it even matter? Now that I finally understand, I feel it is my duty to write it all down, so that any data scientist out there that remains confused and too afraid to ask can finally find some peace of mind.

This post will contain some technical stuff. If it feels new to you, I recommend you take your time to digest the ideas. Take pen(cil) and paper and try to do the derivations yourself. I'm sure that even if you never need to apply Bayesian statistics to your workflow, simply understanding the topic will give you a whole new perspective that will strengthen your intuition about the things you already know.


## What is Bayesian Statistics?

To put it in context, recall that we care about 2 things in data science:
1. Estimating **parameters** of a model, and
2. Making predictions about **unobserved data**.

If you've been xgboosting all over the place, maybe you no longer care about the specific parameters that optimised your model, but you still care (I hope) about point 2.  In Bayesian statistics, one applies Bayes rule to the relationship between parameters, $$w$$, and data, $$D$$. In case you need a reminder, here's what it says:

$$
\underbrace{P(w \vert D)}_{\mathrm{Posterior}} = \underbrace{P(D \vert w)}_{\mathrm{Likelihood}}\, \underbrace{P(w)}_{\mathrm{Prior}} / \underbrace{P(D)}_{\mathrm{Evidence}}
$$

This is where a frequentist is meant to jump out of the bushes and shame you in front of all your friends. For a frequentist, parameters are not random variables so it does not make sense to talk about its probability distributions. But Bayesians are kinder creatures that use distributions to quantify uncertainty. Is the Bayesian point of view correct? Well, that's the wrong question. The correct question is _what is it useful for?_. The Bayesian approach will differ from traditional ML in 2 important aspects:

1. In traditional ML, we use regularisation to control the values of the parameters in your model. In the Bayesian approach, domain knowledge is used to select a **prior distribution** for the parameters.
2. In traditional ML, we make predictions using the "best-fit" values of the parameters. In the Bayesian approach, predictions are made using all of possible values of the parameters that are consistent with the data.

We will cover these two points in this blog. But first, I need to put a toy example in your head.

## A toy example and its Bayesian interpretation
This is how a typical workflow looks like in ML. Say we are given a binary classification task, $$\left\{x_i, y_i\right\}_{i=1}^{N}$$, with the dataset below, where the <span class="orange_dot"></span> have label $$y=1$$ and <span class="blue_dot"></span> have $$y=0$$.
![Binary classification dataset](/assets/images/blog-images/2020-05-26-bayes/dataset.png)


We refrain from taking out our xgboost hammer this time and use simple logistic regression:
```python
clf = LogisticRegression()
clf.fit(X, y)
```

The plot below shows the classification boundaries, with and without regularisation. I think we can both agree that the plot on the right "looks" better. The plot on the left has a very tight boundary which is not very reasonable. Why? Because you have some experience with logistic regression, so you know that a tight boundary is the result of large weights. But large weights (anything above 5) is very rare in real life logistic regression. This is why it never crossed your mind to not regularise your logistic regression (right?). In fact, even if you forgot, the developers of scikit-learn got your back: regularisation is included by default on the scikit-learn implementation.

![Comparison](/assets/images/blog-images/2020-05-26-bayes/logistic_double.png)


Now, even with regularisation, there's something that you should still feel uneasy about. Take a second look at the plots above. Notice how, in both cases, the classification boundary extends to the left and right in a straight line. Far away from the data or close to it, the boundary is just as tight. Take a good hard look at yourself, you know that can't be right. But don't sweat it, we'll fix it later.

### Bayesian interpretation

Let's look into the details of the above example. In logistic regression, like in many other ML tasks, your goal is to find the set of weights, $$w$$, that minimise a cost function, $$C$$. In the particular case of logistic regression, that cost function is

\begin{align}
C(y, x, w) &= \frac{1}{2}w^2  + \sum_{i=1}^N (1 - y_i) \log (1 - f(x_i, w)) + y_i\log f(x_i, w), \tag{1}\label{eq:one}
\end{align}

where $$f(x , w) = 1/(1 + e^{-x\cdot w})$$. In general, we can write the cost function as two terms: a _regulariser_ and an _error_. The regulariser is a term which depends only on the parameters. It takes large values when the parameter take large values. Thus, its only goal is to prevent the "best-fit" parameters from getting too large. The "best-fit" parameters are mostly decided by the error term, which depends on both the parameters and the data.

$$C(x, y, w) = \underbrace{R(w)}_\text{Independent of data} + \underbrace{E(x, y, w)}_\text{Data and weights dependent}. \tag{2}\label{eq:two}$$

This division gives a natural probabilistic interpretation to the process of learning. First, recall that we interpret the output $$f(x, w)$$ literally as the probability that the input $$x$$ has label $$y=1$$. In other words, $$f(x, w) = P(y=1\vert x, w)$$. This can be used to show that the error term is in fact the negative log-likelihood of the parameters:

$$P(y\vert x, w) = \exp(-E(x, y, w))\tag{3}\label{eq:three}$$

Similarly, if we put the regulariser on the same footing, we can interpret it as defining a prior distribution for the parameters:

\begin{align}
P(w) = \frac{1}{Z_R}\exp(-R(w)) \tag{4}\label{eq:four}
\end{align}

The constant $$Z_R$$ is just a normalisation factor to ensure that probabilities integrate to $$1$$, but it does not depend on $$w$$ so it does not "shape" of the distribution. We don't need to worry too much about it. The cost function then defines a posterior distribution for the parameters[^1]:

\begin{align}
P(w\vert x, y) &=\frac{P(y\vert x, w) P(x\vert w) P(w)}{P(x, y)} \quad\quad \text{From Bayes rule}\newline 
&=\frac{P(y\vert x, w) P(x) P(w)}{P(x, y)} \quad\quad \text{Assume } P(x\vert w) = P(x)\newline 
&= \frac{e^{-E(x, y, w)} e^{-R(w)} P(x)}{Z_R P(x, y)}
\end{align}

Finally, if we simply define $$Z_C = Z_R P(y\vert x)$$, then

\begin{align}
P(w\vert x, y) = \frac{1}{Z_C}\exp (-C(x, y, w)). \tag{5}\label{eq:five}
\end{align}

And this is the punchline: 
> The minimum of the cost function is the maximum of the posterior distribution -- a.k.a. the _mode_. 

When you fit your model and get a "best-fit" value for $$w$$, such value is often called the MAP estimate (for _maximum a posteriori_[^2]). If we don't regularise we get $$R(w) = 0$$ which in turn implies that $$P(w) = \text{constant}$$, that is, a uniform distribution. In this case, the posterior distribution is equal (up to a constant factor) to the likelihood, and finding the best $$w$$ corresponds to finding a _maximum likelihood estimate_[^3], a term which I'm sure you've heard before. Let's talk a bit more about that <s>regulariser</s> prior distribution and how it interacts with the posterior.

[^2]: Pro-tip: Whenever you have the opportunity, use Latin for your naming conventions to increase your chances of looking smart.
[^3]: I prefer the term _maximum a likelihoodi_.

<!-- Where, in the third line I've made use of equations \eqref{eq:one} and  -->

## Prior vs Regulariser

Typical regularisation techniques are presented as penalties: L2 penalty, L1 penalty, no penalty, etc. In the toy example above, I used an L2 penalty which is a quadratic function of the weights -- see \eqref{eq:one}. The idea of penalties is to put a heavy cost on large values of the parameters, driving the final estimates closer to zero. From the Bayesian viewpoint however, this L2 penalty corresponds, via equation \eqref{eq:four}, to a prior gaussian distribution centered at zero. The effect is the same, but the interpretation is different and, if I may say, more powerful. In the Bayesian framework you're not limited to use priors that play the role of penalties. A prior distribution is something that you know about your parameters before you see any data. This could be the fact that the weights are small numbers (as is the case of penalties), but you can specify more complex distributions that better reflect your knowledge. For example, you might want to say that a certain parameter is close to $$1$$, instead of close to zero. Or maybe, having negative coefficients would not make sense and you want to constraint your parameters to be positive. Or perhaps, you know that parameter $$w_1$$ should always be greater than $$w_2$$, etc. The Bayesian approach is the way to go when you want to make inferences that respect and exploit your prior knowledge of a subject.

As you collect data, your prior will be updated to reflect the fact that you now have more information. The figure below shows an example of what happens to the prior distribution as you see more data. On the far left, you have no data, so the prior is just a Gaussian centered at zero -- a Gaussian because you are using L2 regularisation. With each new data point, the distribution moves towards the set of parameters that best describe the data. We now call it a _posterior_ distribution. If you take the mode of that posterior distribution and use it to make predictions, you will get the classification boundary that is shown in the top row of the figure. I've marked the mode with a <span class="red_text">★</span>.

![Prior vs Posterior](/assets/images/blog-images/2020-05-26-bayes/prior_posterior.png)

## Making predictions the right way (and the wrong way)

So now that we've seen a couple of data points and their labels, we want to make predictions about new data points. Let's write this carefully, shall we? 
1. We have seen data $$x$$ with its corresponding labels $$y$$,
2. We now have new data $$\tilde{x}$$, but no label. We don't know $$\tilde{y}$$.
3. So we ask what is the probability $$P(\tilde{y}=1\vert \tilde{x}, x, y)$$ ?.

And this is how you get the correct answer using mathematics[^4]:

\begin{align}
P(\tilde{y}\vert \tilde{x}, x, y) &= \frac{P(\tilde{x}, \tilde{y}\vert x, y)}{P(\tilde{x}\vert x, y)}\newline
&=\frac{1}{P(\tilde{x}\vert x, y)}\int\mathrm{d}w P(w, \tilde{x}, \tilde{y}\vert x, y)\newline
&=\frac{1}{P(\tilde{x}\vert x, y)}\int\mathrm{d}w P(\tilde{y}\vert w, \tilde{x}, x, y) P(\tilde{x}\vert w, x, y) P(w\vert x, y)\newline
&=\int\mathrm{d}w P(\tilde{y}\vert w, \tilde{x})  P(w\vert x, y). \tag{6}\label{eq:six}
\end{align}

[^4]: For the last line, we had to assume that the data are conditionally independent given the parameters, $$P(\tilde{y}\vert w, \tilde{x}, x, y) = P(\tilde{y}\vert w, \tilde{x})$$, and (again) that the inputs are not modelled so that $$P(\tilde{x}\vert w, x, y) = P(\tilde{x}\vert x, y) = P(\tilde{x})$$.

Now, the following might come as a shock to you. You'd hope that whenever you take your trained model and do `model.predict_proba(X_test)`, the above integral is being computed to give you the answer. After all, to compute the integral you need to know the likelihood and the posterior, both of which you've already specified. But no. God no. Integrals are difficult and even approximate answers require heavy computation. Instead, the answer you get is

\begin{align}
\texttt{model.predict_proba(X_test)} &= P(\tilde{y} \vert w_{\mathrm{mode}}, \tilde{x}), \tag{7}\label{eq:seven}
\end{align}

where $$w_{\mathrm{mode}}$$ is the mode of the posterior $$P(w\vert x, y)$$. This is the __wrong answer__. The only version of the world in which this answer is correct is if the whole posterior distribution were concentrated in a single point like a Dirac delta distribution. Now, it is true that with more and more data the posterior distribution will indeed get tighter (provided you have adequate regularisation). Despite being technically wrong, using equation \eqref{eq:seven} is often good enough. But the point is that it often isn't. The main reason to use the answer given in \eqref{eq:seven} (which is wrong) instead of the answer given in \eqref{eq:six} (which is correct) is computation. The mode, $$w_{\mathrm{mode}}$$, can be found easily using gradient descent -- taking derivatives is easier than calculating integrals.

Suppose now, that you're not in a rush for once. What would you have to do to compute the correct answer \eqref{eq:seven}? Well, I'm glad you asked. Sometimes the data and model at hand will be simple enough and one will be able to compute the integral analytically[^5]. The real challenge is to develop an algorithm that can compute the integral _exactly_ for any likelihood function. Since the likelihood depends on the data, this is a very difficult problem. _I have a truly marvelous algorithm to do this which this post is to narrow to contain_. So, instead let's talk about how compute the integral _approximately_. These are some options:

[^5]: LOL. Just kidding. This never happens in real life.

- Laplace's approximation.
- Variational inference.
- Monte Carlo methods.

Here I'll tell you about the last one.

## Monte Carlo estimator
Entire books, PhDs, legends, and songs have been written on this topic. Someone even named a casino after it. Needless to say, I'll do no justice to the its complexity. My notes here will be pretty basic just so that we can keep talking about Bayes. If you'd like to read more about it, I strongly recommend you [this out-of-the-oven post by Mike Betancourt](https://betanalpha.github.io/assets/case_studies/markov_chain_monte_carlo.html).

If you look closely at equation \eqref{eq:six} you'll notice that the integral that you have to compute has a meaning: it is the _average_ of the likelihood function under the posterior distribution. 


## Stan

# Bayesian Logistic Regression
<div id="graph"></div>
<script type="module" src="/assets/js/charts/sampler.js"></script>

[^1]: We have to assume here that $$P(x\vert w) = P(x)$$. This means that we do not model our input data. If you are studying a dataset in which there was any sort of selection bias, this assumption would not be true and you'll have to build your model with more care.