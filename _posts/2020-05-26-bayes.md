---
layout: single
title:  "Bayesian statistics for the gradient-based data scientist"
description: "Intro to Bayesian statistics for Machine Learning"
date:   2020-05-26
mathjax: true
plotly: true
tags: [bayesian statistics, python, pystan]
---
Sorry about the title, tho.

<iframe src="https://vlipsy.com/embed/Ru1VsdIv" width="640" height="360" frameborder="0"></iframe>

# What is Bayesian Statistics?
To put it in context, recall that we care about 2 things in data science:
1. Estimating **parameters** of a model, and
2. Making predictions about **unobserved data**.

If you've been xgboosting all over the place, maybe you no longer care about the specific parameters that optimised your model, but you still care (I hope) about point 2. But if you have a soul, you'll also care about point 1.

Bayesian statistics differs from your traditional ML in 2 important aspects:
1. Domain knowlege (or beliefs) are used to select a **prior distribution** for model parameters, and
2. Instead of simply choosing "the best" parameters, predictions are made using all of possible parameters that are consistent with the data.

# Why is it relevant in Machine Learning?
This is how a typical workflow looks like in ML. Say we are given a classification task with the dataset below:

![No regularisation](/assets/images/blog-images/2020-05-26-bayes/dataset.png)

We refrain from taking out our xgboost hammer this time and use simple logistic regression.

```python
clf = LogisticRegression(solver="lbfgs", penalty='none')
clf.fit(X, y)
print(clf.coef_) # Output: [[-10.75027413  22.97515345]]
```
The classification boundary would look like this:
![No regularisation](/assets/images/blog-images/2020-05-26-bayes/logistic_no_regularisation.png)

Of course, this looks rather ridicule. The classification boundary is way too tight. This is, of course, due to not regularising. Check out the massive values of the coefficients, in Logistic regression any coefficient above 5 is most likely an exaggeration. 

So we fit again, this time with bit of regularisation:

```python
clf = LogisticRegression(solver="lbfgs", C=10)
clf.fit(X, y)
print(clf.coef_) # Output: [[-0.73729648  3.16552195]]
```

Now the classification makes more sense:

![No regularisation](/assets/images/blog-images/2020-05-26-bayes/logistic_regularisation.png)


But there's something that you should still feel uneasy about. Take a second look at the above plot. Notice how the classification boundary extends to the left and right in a straigh line. Far away from the data or close to it, the boundary looks the same.


## Cost function as likelihood

## Regularisation as a prior

# Bayesian inference

## Monte Carlo estimator

## Stan

# Bayesian Logistic Regression
<div id="graph"></div>
<script type="module" src="/assets/js/charts/sampler.js"></script>
<!-- <script>
	// TESTER = document.getElementById('tester');
	// Plotly.newPlot( TESTER, [{
	// x: [1, 2, 3, 4, 5],
	// y: [1, 2, 4, 8, 16] }], {
    // margin: { t: 0 } } );
    var myPlot = document.getElementById('myDiv'),
    d3 = Plotly.d3,
    N = 16,
    x = d3.range(N),
    y = d3.range(N).map( d3.random.normal() ),
    data = [ { x:x, y:y, type:'scatter',
            mode:'markers', marker:{size:16} } ],
    layout = {
        hovermode:'closest',
        title:'Click on Points'
     };
    Plotly.newPlot('myDiv', data, layout);
    myPlot.on('plotly_click', function(data){
        var pts = '';
        for(var i=0; i < data.points.length; i++){
            pts = 'x = '+data.points[i].x +'\ny = '+
                data.points[i].y.toPrecision(4) + '\n\n';
        }
        alert('Closest point clicked:\n\n'+pts);
    });
</script> -->
